George Kingsley Zipf, um linguista de Harvard popularizou a distribuição de Zipf, que é a forma discreta da distribuição de Pareto (contínua). A partir da distribuição de Pareto, temos também o princípio de Pareto, que diz que 20% das causas são responsáveis por 80% das consequências (e vice-versa), mas vamos focar na Lei de Zipf por hora.
A Lei de Zipf é expressa em termos de frequência de ocorrência (contagem, quantidade) de eventos. A lei pode ser denotada como:
					F ~ r-a
Onde F é a frequência de ocorrência de um evento, r é seu ranque estatístico, ou seja, sua posição numa lista ordenada dos eventos, e a é próximo de 1. Para melhor ilustrar o funcionamento da Lei, podemos visualizar o exemplo de um livro qualquer, onde cada palavra no livro seria um evento. Digamos que a palavra com o maior número de ocorrências tenha aparecido x vezes, a partir disso, podemos dizer que o número de ocorrências da segunda palavra será próximo de x/2, e para a terceira, x/3 e assim por diante, obtendo a distribuição de Zipf completa do livro.
Um estudo interessante dessa distribuição foi feito por Richard Voss e John Clarke (1975, 1978), voltado para músicas de estações de rádio de rock, blues, jazz e clássica. Eles mediram vários parâmetros, incluindo voltagem de amplificadores, flutuações de volume da música e flutuações de tom da música. Com esses estudos, eles descobriram que tanto as flutuações de tom quanto as de volume seguiam a distribuição de Zipf. Além disso, eles também fizeram um programa para gerar música usando 3 geradores de número aleatório diferentes: uma fonte de white-noise (1/f 0), uma fonte de pink-noise (1/f), e uma fonte de brown-noise (1/f 2). Para controlar a duração das notas e o tom eles usaram geradores de números aleatórios independentes. O resultado deste experimento foi que para a maioria dos ouvintes, a música produzida a partir dos geradores de pink-noise, que segue a distribuição 1/f, foi a mais agradável, enquanto a dos geradores de white-noise era aleatória demais e de brown-noise correlacionada demais.
No âmbito de compressão de arquivos, a lei de Zipf, pode ser usada para modelar a forma de compressão. De acordo com a lei, palavras com um menor número de caracteres tendem a aparecer mais do que palavras com mais caracteres. O mesmo conceito pode ser aplicado para a frequência de ocorrência de cada letra do alfabeto, por exemplo a letra “u”, na língua portuguesa tem uma probabilidade de ocorrência menor que 25%, ou seja, podemos optar por gerar um código para ela com pelo menos 2 bits, dada a baixa probabilidade. Porém depois de uma letra “q” a probabilidade para a letra “u” cresce consideravelmente, nos incentivando a gerar um código para a letra “u” que vier depois da letra “q” mais curto.
Para acabar, um último exemplo da distribuição de Zipf funcionando pode ser observado neste próprio texto, como podemos ver abaixo:
